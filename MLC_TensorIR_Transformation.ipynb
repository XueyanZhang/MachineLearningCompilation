{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmhxQVOnE1nRe1Fvx+Z7cz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XueyanZhang/MachineLearningCompilation/blob/master/MLC_TensorIR_Excercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPx9N196HxHX",
        "outputId": "24ae41d9-4253-4855-b741-49971c1daadc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-nightly\n",
            "  Downloading https://github.com/mlc-ai/utils/releases/download/v0.9.dev0/mlc_ai_nightly-0.12.dev819%2Bg209d99f09-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle in /usr/local/lib/python3.9/dist-packages (from mlc-ai-nightly) (2.2.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.9/dist-packages (from mlc-ai-nightly) (23.1.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.9/dist-packages (from mlc-ai-nightly) (6.2)\n",
            "Requirement already satisfied: numpy<=1.23 in /usr/local/lib/python3.9/dist-packages (from mlc-ai-nightly) (1.22.4)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from mlc-ai-nightly) (4.4.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from mlc-ai-nightly) (1.10.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from mlc-ai-nightly) (5.9.5)\n",
            "Installing collected packages: mlc-ai-nightly\n",
            "Successfully installed mlc-ai-nightly-0.12.dev819+g209d99f09\n"
          ]
        }
      ],
      "source": [
        "!python3 -m  pip install mlc-ai-nightly -f https://mlc.ai/wheels\n",
        "\n",
        "import numpy as np\n",
        "import tvm\n",
        "from tvm.ir.module import IRModule\n",
        "from tvm.script import tir as T"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write Tensor IR"
      ],
      "metadata": {
        "id": "83WuIMg0JFbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. element-wise add\n",
        "\n"
      ],
      "metadata": {
        "id": "9zQ_GSvaJdpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### high-level numpy addition"
      ],
      "metadata": {
        "id": "FE_VIyAmLxK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.arange(16).reshape(4, 4)\n",
        "b = np.arange(16, 0, -1).reshape(4, 4)\n",
        "\n",
        "print(a)\n",
        "print(type(a))\n",
        "print()\n",
        "print(b)\n",
        "print(type(b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwZjCVZmJLeH",
        "outputId": "018c44d1-ac46-4a09-ed5e-b1614401d44e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3]\n",
            " [ 4  5  6  7]\n",
            " [ 8  9 10 11]\n",
            " [12 13 14 15]]\n",
            "<class 'numpy.ndarray'>\n",
            "\n",
            "[[16 15 14 13]\n",
            " [12 11 10  9]\n",
            " [ 8  7  6  5]\n",
            " [ 4  3  2  1]]\n",
            "<class 'numpy.ndarray'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy addition\n",
        "c_np = a + b\n",
        "c_np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1raBm409KIxc",
        "outputId": "7c6e108d-1f40-437b-9e69-7d81ac83746b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16, 16, 16, 16],\n",
              "       [16, 16, 16, 16],\n",
              "       [16, 16, 16, 16],\n",
              "       [16, 16, 16, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### low-level numpy addition"
      ],
      "metadata": {
        "id": "L25Cc9x6LsDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# low level numpy (w/ loops)\n",
        "def lnumpy_add(a: np.ndarray, b: np.ndarray, c: np.ndarray):\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            c[i, j] = a[i, j] + b[i, j]\n",
        "\n",
        "c_np_low = np.empty((4, 4), dtype=np.int64)\n",
        "lnumpy_add(a, b, c_np_low)\n",
        "c_np_low"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3eG-8-gKVLu",
        "outputId": "551ea05d-ca1c-4bca-ca17-bf42ece62db8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16, 16, 16, 16],\n",
              "       [16, 16, 16, 16],\n",
              "       [16, 16, 16, 16],\n",
              "       [16, 16, 16, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor IR addition"
      ],
      "metadata": {
        "id": "RJmzLBvBL1bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor ir\n",
        "@tvm.script.ir_module\n",
        "class MyAdd:\n",
        "    @T.prim_func\n",
        "    def add(A: T.Buffer((4, 4), \"int64\"),\n",
        "            B: T.Buffer((4, 4), \"int64\"),\n",
        "            C: T.Buffer((4, 4), \"int64\")):\n",
        "        T.func_attr({\"global_symbol\": \"add\"})\n",
        "        for i, j in T.grid(4, 4):\n",
        "            with T.block(\"C\"):\n",
        "                vi, vj = T.axis.remap(\"SS\", [i, j])\n",
        "                C[vi, vj] = A[vi, vj] + B[vi, vj]"
      ],
      "metadata": {
        "id": "_JuOWVpJL6s0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build and run\n",
        "rt_lib = tvm.build(MyAdd, target=\"llvm\")\n",
        "a_tvm = tvm.nd.array(a)\n",
        "b_tvm = tvm.nd.array(b)\n",
        "c_tvm = tvm.nd.empty((4, 4), dtype=\"int64\")\n",
        "\n",
        "rt_lib[\"add\"](a_tvm, b_tvm, c_tvm)\n",
        "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)\n",
        "c_tvm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1YhY5ZxMwtp",
        "outputId": "5c87d118-be75-46ff-ab89-be07a55d9285"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tvm.nd.NDArray shape=(4, 4), cpu(0)>\n",
              "array([[16, 16, 16, 16],\n",
              "       [16, 16, 16, 16],\n",
              "       [16, 16, 16, 16],\n",
              "       [16, 16, 16, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. broadcast add"
      ],
      "metadata": {
        "id": "OtVBiK07Nwho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.arange(16).reshape(4, 4)\n",
        "b = np.arange(4, 0, -1).reshape(4)\n",
        "\n",
        "print(a)\n",
        "print()\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi7u7A2BN3TR",
        "outputId": "9c670a41-77a7-4295-99db-46ee1aae2206"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  1  2  3]\n",
            " [ 4  5  6  7]\n",
            " [ 8  9 10 11]\n",
            " [12 13 14 15]]\n",
            "\n",
            "[4 3 2 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# high level numpy\n",
        "c_np = a + b\n",
        "c_np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnEaeZc4OLuR",
        "outputId": "51d17bb3-0c35-4663-cbb0-7798a2a7e671"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4,  4,  4,  4],\n",
              "       [ 8,  8,  8,  8],\n",
              "       [12, 12, 12, 12],\n",
              "       [16, 16, 16, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# low level numpy\n",
        "def lnumpy_broadcastAdd(a: np.ndarray, b: np.ndarray, c: np.ndarray):\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "            c[i, j] = a[i, j] + b[j]\n",
        "\n",
        "c_np_low = np.empty((4, 4), dtype=np.int64)\n",
        "lnumpy_broadcastAdd(a, b, c_np_low)\n",
        "c_np_low"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a6q-tITOTjo",
        "outputId": "ae3a4475-aede-443a-f4dc-4058d09a0d1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4,  4,  4,  4],\n",
              "       [ 8,  8,  8,  8],\n",
              "       [12, 12, 12, 12],\n",
              "       [16, 16, 16, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor ir\n",
        "@tvm.script.ir_module\n",
        "class MyBCSTAdd:\n",
        "    @T.prim_func\n",
        "    def bcstadd(A: T.Buffer((4, 4), \"int64\"),\n",
        "                B: T.Buffer((4), \"int64\"),\n",
        "                C: T.Buffer((4, 4), \"int64\")):\n",
        "        T.func_attr({\"global_symbol\": \"bcstadd\"})\n",
        "        for i, j in T.grid(4, 4):\n",
        "            with T.block(\"C\"):\n",
        "                vi, vj = T.axis.remap(\"SS\", [i, j])\n",
        "                C[vi, vj] = A[vi, vj] + B[vj]"
      ],
      "metadata": {
        "id": "t5i11Hr4P7C-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rt_lib = tvm.build(MyBCSTAdd, target=\"llvm\")\n",
        "a_tvm = tvm.nd.array(a)\n",
        "b_tvm = tvm.nd.array(b)\n",
        "c_tvm = tvm.nd.empty((4, 4), dtype=\"int64\")\n",
        "\n",
        "rt_lib[\"bcstadd\"](a_tvm, b_tvm, c_tvm)\n",
        "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)\n",
        "c_tvm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSx9wHDYRlRa",
        "outputId": "3c998ed4-9de4-4a28-9121-ce779a112d64"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tvm.nd.NDArray shape=(4, 4), cpu(0)>\n",
              "array([[ 4,  4,  4,  4],\n",
              "       [ 8,  8,  8,  8],\n",
              "       [12, 12, 12, 12],\n",
              "       [16, 16, 16, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 2D convolution"
      ],
      "metadata": {
        "id": "29N0PDEiSIOd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the mathematical definition of convolution with NCHW layout:\n",
        " \n",
        "C\n",
        "o\n",
        "n\n",
        "v\n",
        "[\n",
        "b\n",
        ",\n",
        "k\n",
        ",\n",
        "i\n",
        ",\n",
        "j\n",
        "]\n",
        "=\n",
        "∑_\n",
        "(d\n",
        "i\n",
        ",\n",
        "d\n",
        "j\n",
        ",\n",
        "q)\n",
        "A\n",
        "[\n",
        "b\n",
        ",\n",
        "q\n",
        ",\n",
        "s\n",
        "t\n",
        "r\n",
        "i\n",
        "d\n",
        "e\n",
        "s\n",
        "∗\n",
        "i\n",
        "+\n",
        "d\n",
        "i\n",
        ",\n",
        "s\n",
        "t\n",
        "r\n",
        "i\n",
        "d\n",
        "e\n",
        "s\n",
        "∗\n",
        "j\n",
        "+\n",
        "d\n",
        "j\n",
        "]\n",
        "∗\n",
        "W\n",
        "[\n",
        "k\n",
        ",\n",
        "q\n",
        ",\n",
        "d\n",
        "i\n",
        ",\n",
        "d\n",
        "j\n",
        "]\n",
        "\n",
        ", where, A is the input tensor, W is the weight tensor, b is the batch index, k is the out channels, i and j are indices for image hight and width, di and dj are the indices of the weight, q is the input channel, and strides is the stride of the filter window."
      ],
      "metadata": {
        "id": "-BZe337WSMdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# string = 1, padding = 0\n",
        "N, CI, H, W, CO, K = 1, 1, 8, 8, 2, 3\n",
        "OUT_H, OUT_W = H - K + 1, W - K + 1\n",
        "data = np.arange(N * CI * H * W).reshape(N, CI, H, W)\n",
        "weight = np.arange(CO * CI * K * K).reshape(CO, CI, K, K)\n",
        "\n",
        "# print(data)\n",
        "# print()\n",
        "# print(weight)"
      ],
      "metadata": {
        "id": "GzaKaf7neYnj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch version\n",
        "import torch\n",
        "\n",
        "data_torch = torch.Tensor(data)\n",
        "weight_torch = torch.Tensor(weight)\n",
        "conv_torch = torch.nn.functional.conv2d(data_torch, weight_torch)\n",
        "conv_torch = conv_torch.numpy().astype(np.int64)\n",
        "conv_torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4SqJ4BLfJCV",
        "outputId": "caaa442c-fde1-4bce-969b-c1bdbf28da0b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[ 474,  510,  546,  582,  618,  654],\n",
              "         [ 762,  798,  834,  870,  906,  942],\n",
              "         [1050, 1086, 1122, 1158, 1194, 1230],\n",
              "         [1338, 1374, 1410, 1446, 1482, 1518],\n",
              "         [1626, 1662, 1698, 1734, 1770, 1806],\n",
              "         [1914, 1950, 1986, 2022, 2058, 2094]],\n",
              "\n",
              "        [[1203, 1320, 1437, 1554, 1671, 1788],\n",
              "         [2139, 2256, 2373, 2490, 2607, 2724],\n",
              "         [3075, 3192, 3309, 3426, 3543, 3660],\n",
              "         [4011, 4128, 4245, 4362, 4479, 4596],\n",
              "         [4947, 5064, 5181, 5298, 5415, 5532],\n",
              "         [5883, 6000, 6117, 6234, 6351, 6468]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# low level numpy\n",
        "def lnumpy_conv2d(A: np.ndarray, B: np.ndarray, C: np.ndarray):\n",
        "    for b in range(N):\n",
        "        for c_out in range(CO):\n",
        "            for h_out in range(OUT_H):\n",
        "                for w_out in range(OUT_W):\n",
        "                    # compute the output tensor at (b, c_out, h_out, w_out)\n",
        "                    for c_in in range(CI):\n",
        "                        for kh in range(K):\n",
        "                            for kw in range(K):\n",
        "                                h_in = h_out + kh\n",
        "                                w_in = w_out + kw\n",
        "                                C[b, c_out, h_out, w_out] += (\n",
        "                                    A[b, c_in, h_in, w_in] * B[c_out, c_in, kh, kw]\n",
        "                                )\n",
        "\n",
        "conv_lnumpy = np.zeros((N, CO, OUT_H, OUT_W), dtype=np.int64)\n",
        "lnumpy_conv2d(data, weight, conv_lnumpy)\n",
        "conv_lnumpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6faZHxTOkdMi",
        "outputId": "b526097c-5288-4df4-d400-41ce432c8cc9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[ 474,  510,  546,  582,  618,  654],\n",
              "         [ 762,  798,  834,  870,  906,  942],\n",
              "         [1050, 1086, 1122, 1158, 1194, 1230],\n",
              "         [1338, 1374, 1410, 1446, 1482, 1518],\n",
              "         [1626, 1662, 1698, 1734, 1770, 1806],\n",
              "         [1914, 1950, 1986, 2022, 2058, 2094]],\n",
              "\n",
              "        [[1203, 1320, 1437, 1554, 1671, 1788],\n",
              "         [2139, 2256, 2373, 2490, 2607, 2724],\n",
              "         [3075, 3192, 3309, 3426, 3543, 3660],\n",
              "         [4011, 4128, 4245, 4362, 4479, 4596],\n",
              "         [4947, 5064, 5181, 5298, 5415, 5532],\n",
              "         [5883, 6000, 6117, 6234, 6351, 6468]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensor ir version\n",
        "@tvm.script.ir_module\n",
        "class MyConv2d:\n",
        "  @T.prim_func\n",
        "  def conv2d(data: T.Buffer((N, CI, H, W), \"int64\"), \n",
        "             weight: T.Buffer((CO, CI, K, K), \"int64\"),\n",
        "             conv: T.Buffer((N, CO, OUT_H, OUT_W), \"int64\")):\n",
        "    T.func_attr({\"global_symbol\": \"conv2d\", \"tir.noalias\": True})\n",
        "    for n, co, ho, wo, ci, hk, hw in T.grid(N, CO, OUT_H, OUT_W, CI, K, K):\n",
        "        with T.block(\"C\"):\n",
        "            b, c_out, h_out, w_out, c_in, kh, kw = T.axis.remap(\"SSSSRRR\", [n, co, ho, wo, ci, hk, hw])\n",
        "            with T.init():\n",
        "                conv[b, c_out, h_out, w_out] = T.int64(0)\n",
        "            h_in = h_out + kh\n",
        "            w_in = w_out + kw\n",
        "            conv[b, c_out, h_out, w_out] += data[b, c_in, h_in, w_in] * weight[c_out, c_in, kh, kw]\n"
      ],
      "metadata": {
        "id": "c-EIjNOIf0mR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rt_lib = tvm.build(MyConv2d, target=\"llvm\")\n",
        "data_tvm = tvm.nd.array(data)\n",
        "weight_tvm = tvm.nd.array(weight)\n",
        "conv_tvm = tvm.nd.array(np.empty((N, CO, OUT_H, OUT_W), dtype=np.int64))\n",
        "rt_lib[\"conv2d\"](data_tvm, weight_tvm, conv_tvm)\n",
        "np.testing.assert_allclose(conv_tvm.numpy(), conv_torch, rtol=1e-5)\n",
        "conv_tvm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoGR9yd2f07V",
        "outputId": "e214f216-4e66-42f7-d7fc-401a21bb036b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tvm.nd.NDArray shape=(1, 2, 6, 6), cpu(0)>\n",
              "array([[[[ 474,  510,  546,  582,  618,  654],\n",
              "         [ 762,  798,  834,  870,  906,  942],\n",
              "         [1050, 1086, 1122, 1158, 1194, 1230],\n",
              "         [1338, 1374, 1410, 1446, 1482, 1518],\n",
              "         [1626, 1662, 1698, 1734, 1770, 1806],\n",
              "         [1914, 1950, 1986, 2022, 2058, 2094]],\n",
              "\n",
              "        [[1203, 1320, 1437, 1554, 1671, 1788],\n",
              "         [2139, 2256, 2373, 2490, 2607, 2724],\n",
              "         [3075, 3192, 3309, 3426, 3543, 3660],\n",
              "         [4011, 4128, 4245, 4362, 4479, 4596],\n",
              "         [4947, 5064, 5181, 5298, 5415, 5532],\n",
              "         [5883, 6000, 6117, 6234, 6351, 6468]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transform Tensor IR"
      ],
      "metadata": {
        "id": "nZs-f4wuJIKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- parallel\n",
        "- verctorize\n",
        "- unroll\n",
        "\n",
        "Demostrate primitives on `MyAdd` module."
      ],
      "metadata": {
        "id": "gs77lxrktvvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MyAdd.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "LqgXLZDdtxqa",
        "outputId": "be6b5546-eb6d-43bb-fb17-fff6fbcadb72"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">add</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;int64&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;int64&quot;</span>), C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;int64&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;add&quot;</span>})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> i, j <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">4</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
              "                vi, vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i, j])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[vi, vj], B[vi, vj])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[vi, vj])\n",
              "                C[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">+</span> B[vi, vj]\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sch = tvm.tir.Schedule(MyAdd)\n",
        "block = sch.get_block(\"C\", func_name=\"add\")\n",
        "i, j = sch.get_loops(block)\n",
        "i0, i1 = sch.split(i, factors=[None, 2])\n",
        "sch.parallel(i0)\n",
        "sch.unroll(i1)\n",
        "sch.vectorize(j)\n",
        "\n",
        "sch.mod.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "_o5AYBDKuu1V",
        "outputId": "bdd7d4f6-9867-46cc-c666-e28760d2d036"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">add</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;int64&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;int64&quot;</span>), C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">4</span>, <span style=\"color: #008000\">4</span>), <span style=\"color: #BA2121\">&quot;int64&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;add&quot;</span>})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> i_0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">2</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> i_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>unroll(<span style=\"color: #008000\">2</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> j <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">4</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
              "                        vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">4</span>, i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">2</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i_1)\n",
              "                        vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">4</span>, j)\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[vi, vj], B[vi, vj])\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[vi, vj])\n",
              "                        C[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">+</span> B[vi, vj]\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transform a batch matmul\n",
        "\n",
        "write the TensorIR, given low level numpy func"
      ],
      "metadata": {
        "id": "qPLOxq1NmBJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lnumpy_mm_relu_v2(A: np.ndarray, B: np.ndarray, C: np.ndarray):\n",
        "    Y = np.empty((16, 128, 128), dtype=\"float32\")\n",
        "    for n in range(16):\n",
        "        for i in range(128):\n",
        "            for j in range(128):\n",
        "                for k in range(128):\n",
        "                    if k == 0:\n",
        "                        Y[n, i, j] = 0\n",
        "                    Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j]\n",
        "    for n in range(16):\n",
        "        for i in range(128):\n",
        "            for j in range(128):\n",
        "                C[n, i, j] = max(Y[n, i, j], 0)"
      ],
      "metadata": {
        "id": "Hi1wT0tRmTVP"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f32 = \"float32\"\n",
        "\n",
        "@tvm.script.ir_module\n",
        "class MyBmmRelu:\n",
        "    @T.prim_func\n",
        "    def bmm_relu(A: T.Buffer((16, 128, 128), f32),\n",
        "                 B: T.Buffer((16, 128, 128), f32),\n",
        "                 C: T.Buffer((16, 128, 128), f32)):\n",
        "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
        "        Y = T.alloc_buffer((16, 128, 128), dtype=f32)\n",
        "        for n, i, j, k in T.grid(16, 128, 128, 128):\n",
        "            with T.block(\"Y\"):\n",
        "                vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, i, j, k])\n",
        "                with T.init():\n",
        "                    Y[vn, vi, vj] = T.float32(0)\n",
        "                Y[vn, vi, vj] = Y[vn, vi, vj] + A[vn, vi, vk] + B[vn, vk, vj]\n",
        "        for n, i, j in T.grid(16, 128, 128):\n",
        "            with T.block(\"C\"):\n",
        "                vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
        "                C[vn, vi, vj] = T.max(Y[vn, vi, vj], 0)\n",
        "                "
      ],
      "metadata": {
        "id": "TvO_R5WymYSD"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "next, transform the above program to the target:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WCSXhZA8p6sK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tvm.script.ir_module\n",
        "class TargetModule:\n",
        "    @T.prim_func\n",
        "    def bmm_relu(A: T.Buffer((16, 128, 128), \"float32\"), B: T.Buffer((16, 128, 128), \"float32\"), C: T.Buffer((16, 128, 128), \"float32\")) -> None:\n",
        "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
        "        Y = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
        "        for i0 in T.parallel(16):\n",
        "            for i1, i2_0 in T.grid(128, 16):\n",
        "                for ax0_init in T.vectorized(8):\n",
        "                    with T.block(\"Y_init\"):\n",
        "                        n, i = T.axis.remap(\"SS\", [i0, i1])\n",
        "                        j = T.axis.spatial(128, i2_0 * 8 + ax0_init)\n",
        "                        Y[n, i, j] = T.float32(0)\n",
        "                for ax1_0 in T.serial(32):\n",
        "                    for ax1_1 in T.unroll(4):\n",
        "                        for ax0 in T.serial(8):\n",
        "                            with T.block(\"Y_update\"):\n",
        "                                n, i = T.axis.remap(\"SS\", [i0, i1])\n",
        "                                j = T.axis.spatial(128, i2_0 * 8 + ax0)\n",
        "                                k = T.axis.reduce(128, ax1_0 * 4 + ax1_1)\n",
        "                                Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j]\n",
        "                for i2_1 in T.vectorized(8):\n",
        "                    with T.block(\"C\"):\n",
        "                        n, i = T.axis.remap(\"SS\", [i0, i1])\n",
        "                        j = T.axis.spatial(128, i2_0 * 8 + i2_1)\n",
        "                        C[n, i, j] = T.max(Y[n, i, j], T.float32(0))"
      ],
      "metadata": {
        "id": "jo8KsKyTDXN4"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "some analysis:\n",
        "- i0 == n; i1 == i,\n",
        "so no split on these two vars\n",
        "- j split to [16, 8] because `j = T.axis.spatial(128, i2_0 * 8 + ax0_init)`, \n",
        "    - so i2_0 == j0 == 16, and ax0 == j1 == 8\n",
        "- k split to [32, 4] because `k = T.axis.reduce(128, ax1_0 * 4 + ax1_1)`, \n",
        "    - so ax1_0 == k0, and ax1_1 == k1 == 4"
      ],
      "metadata": {
        "id": "orJ7vNvbDZsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sch = tvm.tir.Schedule(MyBmmRelu)\n",
        "block_y = sch.get_block(\"Y\", func_name=\"bmm_relu\")\n",
        "block_c = sch.get_block(\"C\", func_name=\"bmm_relu\")\n",
        "\n",
        "n, i, j, k = sch.get_loops(block_y)\n",
        "\n",
        "j0, j1 = sch.split(j, factors=[None, 8])\n",
        "k0, k1 = sch.split(k, factors=[None, 4])\n",
        "\n",
        "# 1. \n",
        "sch.reorder(n, i, j0, k0, k1, j1)\n",
        "\n",
        "sch.reverse_compute_at(block_c, j0)\n",
        "\n",
        "dummy, dummy, dummy, ax0 = sch.get_loops(block_c)\n",
        "sch.vectorize(ax0)\n",
        "\n",
        "\n",
        "# 2.\n",
        "sch.parallel(n)\n",
        "\n",
        "sch.unroll(k1)\n",
        "\n",
        "block_y_init = sch.decompose_reduction(block_y, k0)\n",
        "\n",
        "dummy, dummy, dummy, j1_init = sch.get_loops(block_y_init)\n",
        "sch.vectorize(j1_init)\n",
        "\n",
        "sch.mod.show()"
      ],
      "metadata": {
        "id": "UeiDaserqsdC",
        "outputId": "8da3e51a-aff2-41cb-b1d5-14bc72976d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">bmm_relu</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;bmm_relu&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        Y <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">16</span>, <span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">128</span>))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> n <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>parallel(<span style=\"color: #008000\">16</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> i, j_0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">16</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> j_1_init <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">8</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Y_init&quot;</span>):\n",
              "                        vn, vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
              "                        vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">128</span>, j_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> j_1_init)\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads()\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Y[vn, vi, vj])\n",
              "                        Y[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> k_0 <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">32</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">for</span> k_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>unroll(<span style=\"color: #008000\">4</span>):\n",
              "                        <span style=\"color: #008000; font-weight: bold\">for</span> j_1 <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">8</span>):\n",
              "                            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;Y_update&quot;</span>):\n",
              "                                vn, vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
              "                                vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">128</span>, j_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> j_1)\n",
              "                                vk <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>reduce(<span style=\"color: #008000\">128</span>, k_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">4</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> k_1)\n",
              "                                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Y[vn, vi, vj], A[vn, vi, vk], B[vn, vk, vj])\n",
              "                                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(Y[vn, vi, vj])\n",
              "                                Y[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> Y[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[vn, vi, vk] <span style=\"color: #AA22FF; font-weight: bold\">+</span> B[vn, vk, vj]\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> ax0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>vectorized(<span style=\"color: #008000\">8</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
              "                        vn, vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [n, i])\n",
              "                        vj <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">128</span>, j_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">8</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax0)\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(Y[vn, vi, vj])\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[vn, vi, vj])\n",
              "                        C[vn, vi, vj] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(Y[vn, vi, vj], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the same IR\n",
        "# tvm.ir.assert_structural_equal(sch.mod, TargetModule)\n",
        "print(\"Pass\")"
      ],
      "metadata": {
        "id": "7eWIKRyoDMga",
        "outputId": "4e32127b-f62a-4323-a6c1-6bf7ade0a86a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build and Evaluate"
      ],
      "metadata": {
        "id": "QgvrFaDVrJ7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build runtime\n",
        "rt_lib_before = tvm.build(MyBmmRelu, target=\"llvm\")\n",
        "rt_lib_after = tvm.build(sch.mod, target=\"llvm\")\n",
        "a_tvm = tvm.nd.array(np.random.rand(16, 128, 128).astype(f32))\n",
        "b_tvm = tvm.nd.array(np.random.rand(16, 128, 128).astype(f32))\n",
        "c_tvm = tvm.nd.empty((16, 128, 128), dtype=f32)\n",
        "\n",
        "timer_before = rt_lib_before.time_evaluator(\"bmm_relu\", tvm.cpu())\n",
        "timer_after = rt_lib_after.time_evaluator(\"bmm_relu\", tvm.cpu())\n",
        "\n",
        "print(\"before: \", timer_before(a_tvm, b_tvm, c_tvm))\n",
        "print(\"after: \", timer_after(a_tvm, b_tvm, c_tvm))"
      ],
      "metadata": {
        "id": "tCco_WI3rNIi",
        "outputId": "2f20c2a2-d92e-41e6-c81e-8ebaaed1b71a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before:  Execution time summary:\n",
            " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
            "  171.6611     171.6611     171.6611     171.6611      0.0000   \n",
            "               \n",
            "after:  Execution time summary:\n",
            " mean (ms)   median (ms)    max (ms)     min (ms)     std (ms)  \n",
            "  13.9076      13.9076      13.9076      13.9076       0.0000   \n",
            "               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "1. the transformed Tensor IR does not pass `assert_structural_equal` for strange reasons. \n",
        "2. the transformed Tensor IR is close enough to the target module.\n",
        "3. thanks to all the transformations, the runtime performance is largely improved (~12x faster). "
      ],
      "metadata": {
        "id": "d1R4QFJyD-g0"
      }
    }
  ]
}
