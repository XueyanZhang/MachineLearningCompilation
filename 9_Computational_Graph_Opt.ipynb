{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNGkD4SBaMUhnxr5rmraREl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XueyanZhang/MachineLearningCompilation/blob/master/9_Computational_Graph_Opt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# High-level transformations among computational graphs."
      ],
      "metadata": {
        "id": "u2rxKrhNkz1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m  pip install mlc-ai-nightly -f https://mlc.ai/wheels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iufNaiJTk2OT",
        "outputId": "566278cc-7492-4a80-ba5f-bb90809a55a0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-nightly\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly-0.12.dev1198-cp310-cp310-manylinux_2_28_x86_64.whl (98.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (23.1.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (2.2.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (0.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (1.22.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (1.10.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (6.3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly) (4.6.3)\n",
            "Installing collected packages: mlc-ai-nightly\n",
            "Successfully installed mlc-ai-nightly-0.12.dev1198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is needed for deferring annotation parsing in TVMScript\n",
        "from __future__ import annotations\n",
        "\n",
        "import tvm\n",
        "from tvm.ir.module import IRModule\n",
        "from tvm.script import tir as T, relax as R\n",
        "from tvm import relax, topi\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "szkHJoTKk5PP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pattern Match and Rewrite\n",
        "FIRST we need to recognize the target code (Pattern Match) and then transform (Rewrite).\n",
        "\n",
        "Given `MyModule` below with two high-level operators, `multiply` and `add`, our goal is to replace them with a single call, `ewise_fma`."
      ],
      "metadata": {
        "id": "LUfneRT_lD_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tvm.script.ir_module\n",
        "class MyModule:\n",
        "    @R.function\n",
        "    def main(x: R.Tensor((3, 4), 'float32'), y: R.Tensor((3, 4), 'float32')):\n",
        "        with R.dataflow():\n",
        "            lv0 = relax.op.multiply(x, y)\n",
        "            gv0 = relax.op.add(lv0, y)\n",
        "            R.output(gv0)\n",
        "        return gv0"
      ],
      "metadata": {
        "id": "7L3l2LfLl61Q"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relax_func = MyModule['main']\n",
        "type(relax_func)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3dQBReSmeNf",
        "outputId": "9deea833-82d4-4177-a26c-89281a971ee9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tvm.relax.expr.Function"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relax_func.params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LuwekzLmnqO",
        "outputId": "2a06bb0d-f88b-4f7e-99d1-f4ade3acac65"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[x, y]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "func_body = relax_func.body\n",
        "type(func_body)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XI6io5Smq2i",
        "outputId": "0418f86b-629b-4bca-ea1b-18e74c543b69"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tvm.relax.expr.SeqExpr"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "func_body.blocks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Tjj4ziSmw3P",
        "outputId": "1a5545e8-f364-48f6-88a1-4df8f310e80a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[x: R.Tensor((3, 4), dtype=\"float32\")\n",
              "y: R.Tensor((3, 4), dtype=\"float32\")\n",
              "with R.dataflow():\n",
              "    lv0: R.Tensor((3, 4), dtype=\"float32\") = R.multiply(x, y)\n",
              "    gv0: R.Tensor((3, 4), dtype=\"float32\") = R.add(lv0, y)\n",
              "    R.output(gv0)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# single block\n",
        "dataflow_block = func_body.blocks[0]"
      ],
      "metadata": {
        "id": "3vy6llz9my1P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataflow_block.bindings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yr_XL3yRnx-_",
        "outputId": "81c32c6b-6533-4de0-ae72-d15f77768c7b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[x: R.Tensor((3, 4), dtype=\"float32\")\n",
              "y: R.Tensor((3, 4), dtype=\"float32\")\n",
              "lv0: R.Tensor((3, 4), dtype=\"float32\") = R.multiply(x, y), lv0: R.Tensor((3, 4), dtype=\"float32\")\n",
              "y: R.Tensor((3, 4), dtype=\"float32\")\n",
              "gv0: R.Tensor((3, 4), dtype=\"float32\") = R.add(lv0, y)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "binding = dataflow_block.bindings[0]\n",
        "binding.var"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_biExQWn5Mv",
        "outputId": "a289e5cf-dac6-4c7f-ec40-752c88b195a1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lv0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "binding2 = dataflow_block.bindings[1]\n",
        "binding2.var"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34n6Hn5toA6P",
        "outputId": "5c4bc691-3a49-4643-d2ea-e0c796e4fe58"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gv0"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "binding.value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVjXlb3WoGhg",
        "outputId": "9b030afa-1ee4-42da-8a0b-a954adcf810c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "R.multiply(x, y)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visitor pattern\n",
        "\n",
        "a design pattern allowing users to visit each AST node and rewrite them to transformed versions."
      ],
      "metadata": {
        "id": "IBskNm6qoSRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@relax.expr_functor.mutator\n",
        "class EwiseFMARewriter(relax.PyExprMutator):\n",
        "    def visit_call_(self, call):\n",
        "        call = self.visit_expr_post_order(call)\n",
        "        add_op = tvm.ir.Op.get(\"relax.add\")\n",
        "        multiply_op = tvm.ir.Op.get(\"relax.multiply\")\n",
        "        ewise_fma_op = tvm.ir.Op.get(\"relax.ewise_fma\")\n",
        "\n",
        "        if call.op != add_op:\n",
        "            return call\n",
        "\n",
        "        value = self.lookup_binding(call.args[0])\n",
        "        if not isinstance(value, relax.Call) or value.op != multiply_op:\n",
        "            return call\n",
        "\n",
        "        fma_call = relax.Call(\n",
        "            ewise_fma_op, [value.args[0], value.args[1], call.args[1]], None, None\n",
        "        )\n",
        "        return fma_call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "QjvIO6Uxod7v",
        "outputId": "7b7a44b4-9914-40c6-99b4-20e583c25dea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
              "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), y: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
              "    <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
              "        lv0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>multiply(x, y)\n",
              "        gv0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>ewise_fma(x, y, y)\n",
              "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv0)\n",
              "    <span style=\"color: #008000; font-weight: bold\">return</span> gv0\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# update add to ewise_fma\n",
        "updated_fn = EwiseFMARewriter().visit_expr(MyModule[\"main\"])\n",
        "updated_fn.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "8TATgMh2qHzP",
        "outputId": "ee1ab6f9-b36b-4f0b-cb70-35ca292ef535"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
              "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), y: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
              "    <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
              "        lv0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>multiply(x, y)\n",
              "        gv0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>ewise_fma(x, y, y)\n",
              "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv0)\n",
              "    <span style=\"color: #008000; font-weight: bold\">return</span> gv0\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove mutiply op\n",
        "relax.analysis.remove_all_unused(updated_fn).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "o2nzzCklrHl9",
        "outputId": "153f24d7-362a-49d2-98ea-796a04f97871"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
              "<span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), y: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
              "    <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
              "        gv0: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">3</span>, <span style=\"color: #008000\">4</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>ewise_fma(x, y, y)\n",
              "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv0)\n",
              "    <span style=\"color: #008000; font-weight: bold\">return</span> gv0\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fuse Linear and ReLU end-to-end"
      ],
      "metadata": {
        "id": "ryYNZLuWrViZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hide outputs\n",
        "!wget https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_params.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWe1bYxprZse",
        "outputId": "6bb9a716-9753-4ee3-d00e-f5b1cf846242"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-01 13:39:03--  https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_params.pkl\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/mlc-ai/web-data/main/models/fasionmnist_mlp_params.pkl [following]\n",
            "--2023-07-01 13:39:04--  https://raw.githubusercontent.com/mlc-ai/web-data/main/models/fasionmnist_mlp_params.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 407396 (398K) [application/octet-stream]\n",
            "Saving to: ‘fasionmnist_mlp_params.pkl’\n",
            "\n",
            "fasionmnist_mlp_par 100%[===================>] 397.85K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-07-01 13:39:04 (8.78 MB/s) - ‘fasionmnist_mlp_params.pkl’ saved [407396/407396]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle as pkl\n",
        "\n",
        "mlp_params = pkl.load(open(\"fasionmnist_mlp_params.pkl\", \"rb\"))"
      ],
      "metadata": {
        "id": "XV5iQtlLrb7f"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP model - fashionMNIST"
      ],
      "metadata": {
        "id": "cn8zIuxZrfTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    bb = relax.BlockBuilder()\n",
        "    x = relax.Var(\"x\", relax.TensorStructInfo((1, 784), \"float32\"))\n",
        "    w0 = relax.const(mlp_params[\"w0\"], \"float32\")\n",
        "    b0 = relax.const(mlp_params[\"b0\"], \"float32\")\n",
        "    w1 = relax.const(mlp_params[\"w1\"], \"float32\")\n",
        "    b1 = relax.const(mlp_params[\"b1\"], \"float32\")\n",
        "    with bb.function(\"main\", [x]):\n",
        "        with bb.dataflow():\n",
        "            lv0 = bb.emit(relax.op.matmul(x, relax.op.permute_dims(w0)))\n",
        "            lv1 = bb.emit(relax.op.add(lv0, b0))\n",
        "            lv2 = bb.emit(relax.op.nn.relu(lv1))\n",
        "            lv3 = bb.emit(relax.op.matmul(lv2, relax.op.permute_dims(w1)))\n",
        "            lv4 = bb.emit(relax.op.add(lv3, b1))\n",
        "            gv = bb.emit_output(lv4)\n",
        "        bb.emit_func_output(gv)\n",
        "\n",
        "    return bb.get()\n",
        "\n",
        "MLPModel = create_model()\n",
        "MLPModel.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "x4b3fzI4rjLP",
        "outputId": "c64177ea-a0df-4926-916b-3036fc576a8e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
              "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
              "            lv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>], axes<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
              "            lv1: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(x, lv, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
              "            lv2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv1, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>])\n",
              "            lv3: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv2)\n",
              "            lv4: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>], axes<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
              "            lv5: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(lv3, lv4, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
              "            lv6: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv5, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>])\n",
              "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv6\n",
              "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
              "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
              "\n",
              "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal: fuse matmul and add\n",
        "\n",
        "Steps:\n",
        "1. identify matmal and add\n",
        "2. fuse\n",
        "3. replace"
      ],
      "metadata": {
        "id": "sD4CGH4rrt94"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@relax.expr_functor.mutator\n",
        "class MatmulAddFusor(relax.PyExprMutator):\n",
        "    def __init__(self, mod: IRModule) -> None:\n",
        "        super().__init__()\n",
        "        self.mod_ = mod\n",
        "        # cache pre-defined ops\n",
        "        self.add_op = tvm.ir.Op.get(\"relax.add\")\n",
        "        self.matmul_op = tvm.ir.Op.get(\"relax.matmul\")\n",
        "        self.counter = 0\n",
        "\n",
        "    def transform(self) -> IRModule:\n",
        "        for global_var, func in self.mod_.functions.items():\n",
        "            if not isinstance(func, relax.Function):\n",
        "                continue\n",
        "            # avoid already fused primitive functions\n",
        "            if func.attrs is not None and \"Primitive\" in func.attrs.keys() and func.attrs[\"Primitive\"] != 0:\n",
        "                continue\n",
        "            updated_func = self.visit_expr(func)\n",
        "            updated_func = relax.analysis.remove_all_unused(updated_func)\n",
        "            self.builder_.update_func(global_var, updated_func)\n",
        "\n",
        "        return self.builder_.get()\n",
        "\n",
        "    def visit_call_(self, call):\n",
        "        call = self.visit_expr_post_order(call)\n",
        "\n",
        "        def match_call(node, op):\n",
        "            if not isinstance(node, relax.Call):\n",
        "                return False\n",
        "            return node.op == op\n",
        "\n",
        "        # pattern match matmul => add\n",
        "        if not match_call(call, self.add_op):\n",
        "            return call\n",
        "\n",
        "        value = self.lookup_binding(call.args[0])\n",
        "        if value is None:\n",
        "            return call\n",
        "\n",
        "        if not match_call(value, self.matmul_op):\n",
        "            return call\n",
        "\n",
        "        x = value.args[0]\n",
        "        w = value.args[1]\n",
        "        b = call.args[1]\n",
        "\n",
        "        # construct a new fused primitive function\n",
        "        param_x = relax.Var(\"x\" ,relax.TensorStructInfo(x.struct_info.shape, x.struct_info.dtype))\n",
        "        param_w = relax.Var(\"w\" ,relax.TensorStructInfo(w.struct_info.shape, w.struct_info.dtype))\n",
        "        param_b = relax.Var(\"b\" ,relax.TensorStructInfo(b.struct_info.shape, b.struct_info.dtype))\n",
        "\n",
        "        bb = relax.BlockBuilder()\n",
        "\n",
        "        fn_name = \"fused_matmul_add%d\" % (self.counter)\n",
        "        self.counter += 1\n",
        "        with bb.function(fn_name, [param_x, param_w, param_b]):\n",
        "            with bb.dataflow():\n",
        "                lv0 = bb.emit(relax.op.matmul(param_x, param_w))\n",
        "                gv = bb.emit_output(relax.op.add(lv0, param_b))\n",
        "            bb.emit_func_output(gv)\n",
        "\n",
        "        # Add Primitive attribute to the fused funtions\n",
        "        fused_fn = bb.get()[fn_name].with_attr(\"Primitive\", 1)\n",
        "        global_var = self.builder_.add_func(fused_fn, fn_name)\n",
        "\n",
        "        # construct call into the fused function\n",
        "        return relax.Call(global_var, [x, w, b], None, None)\n",
        "\n",
        "@tvm.ir.transform.module_pass(opt_level=2, name=\"MatmulAddFuse\")\n",
        "class FuseDenseAddPass:\n",
        "    \"\"\"The wrapper for the LowerTensorIR pass.\"\"\"\n",
        "    def transform_module(self, mod, ctx):\n",
        "        return MatmulAddFusor(mod).transform()"
      ],
      "metadata": {
        "id": "ZNXA_6tDr-Uu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLPFused = FuseDenseAddPass()(MLPModel)\n",
        "MLPFused.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "7nYLZabryuhf",
        "outputId": "a6a69274-2a95-4aea-c497-1b492eb08800"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">fused_matmul_add0</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
              "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
              "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
              "            lv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(x, w, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
              "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv, b)\n",
              "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
              "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">fused_matmul_add1</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
              "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
              "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
              "            lv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul(x, w, out_dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;void&quot;</span>)\n",
              "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>add(lv, b)\n",
              "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
              "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
              "        cls <span style=\"color: #AA22FF; font-weight: bold\">=</span> Module\n",
              "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
              "            lv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>], axes<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
              "            lv2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>fused_matmul_add0(x, lv, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>])\n",
              "            lv3: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>nn<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu(lv2)\n",
              "            lv4: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>permute_dims(metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>], axes<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #008000; font-weight: bold\">None</span>)\n",
              "            lv6: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>fused_matmul_add1(lv3, lv4, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>])\n",
              "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv6\n",
              "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
              "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
              "\n",
              "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lower to TensorIR\n",
        "Translate high-level primitive operators to low-level"
      ],
      "metadata": {
        "id": "zp354YK26KFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@relax.expr_functor.mutator\n",
        "class LowerToTensorIR(relax.PyExprMutator):\n",
        "    def __init__(self, mod: IRModule, op_map) -> None:\n",
        "        super().__init__()\n",
        "        self.mod_ = mod\n",
        "        self.op_map = {\n",
        "            tvm.ir.Op.get(k): v for k, v in op_map.items()\n",
        "        }\n",
        "\n",
        "    def visit_call_(self, call):\n",
        "        call = self.visit_expr_post_order(call)\n",
        "\n",
        "        if call.op in self.op_map:\n",
        "            return self.op_map[call.op](self.builder_, call)\n",
        "        return call\n",
        "\n",
        "    def transform(self) -> IRModule:\n",
        "        for global_var, func in self.mod_.functions.items():\n",
        "            if not isinstance(func, relax.Function):\n",
        "                continue\n",
        "            updated_func = self.visit_expr(func)\n",
        "            self.builder_.update_func(global_var, updated_func)\n",
        "        return self.builder_.get()\n",
        "\n",
        "def map_matmul(bb, call):\n",
        "    x, w = call.args\n",
        "    return bb.call_te(topi.nn.matmul, x, w)\n",
        "\n",
        "def map_add(bb, call):\n",
        "    a, b = call.args\n",
        "    return bb.call_te(topi.add, a, b)\n",
        "\n",
        "def map_relu(bb, call):\n",
        "    return bb.call_te(topi.nn.relu, call.args[0])\n",
        "\n",
        "def map_transpose(bb, call):\n",
        "    return bb.call_te(topi.transpose, call.args[0], )\n",
        "\n",
        "op_map = {\n",
        "  \"relax.matmul\": map_matmul,\n",
        "  \"relax.add\": map_add,\n",
        "  \"relax.nn.relu\": map_relu,\n",
        "  \"relax.permute_dims\": map_transpose\n",
        "}\n",
        "\n",
        "@tvm.ir.transform.module_pass(opt_level=0, name=\"LowerToTensorIR\")\n",
        "class LowerToTensorIRPass:\n",
        "    \"\"\"The wrapper for the LowerTensorIR pass.\"\"\"\n",
        "    def transform_module(self, mod, ctx):\n",
        "        return LowerToTensorIR(mod, op_map).transform()"
      ],
      "metadata": {
        "id": "PxTPLcMb6ua_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLPModelTIR = LowerToTensorIRPass()(MLPFused)\n",
        "MLPModelTIR.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uUIXF4fk76as",
        "outputId": "075a6845-eeb2-4fe8-bdaf-fc36c69f86b3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">add</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_add: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
              "                v_ax0, v_ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_ax0, v_ax1], B[v_ax1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_add[v_ax0, v_ax1])\n",
              "                T_add[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> B[v_ax1]\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">add1</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_add: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
              "                v_ax0, v_ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_ax0, v_ax1], B[v_ax1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_add[v_ax0, v_ax1])\n",
              "                T_add[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> B[v_ax1]\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">matmul</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_matmul_NN: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;layout_free_buffers&quot;</span>: [<span style=\"color: #008000\">1</span>], <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NN&quot;</span>):\n",
              "                v_i, v_j, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_i, v_k], B[v_k, v_j])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_matmul_NN[v_i, v_j])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    T_matmul_NN[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
              "                T_matmul_NN[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T_matmul_NN[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[v_i, v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[v_k, v_j]\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">matmul1</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_matmul_NN: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;layout_free_buffers&quot;</span>: [<span style=\"color: #008000\">1</span>], <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NN&quot;</span>):\n",
              "                v_i, v_j, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_i, v_k], B[v_k, v_j])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_matmul_NN[v_i, v_j])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    T_matmul_NN[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
              "                T_matmul_NN[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T_matmul_NN[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[v_i, v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> B[v_k, v_j]\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">relu</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), compute: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;compute&quot;</span>):\n",
              "                v_i0, v_i1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_i0, v_i1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(compute[v_i0, v_i1])\n",
              "                compute[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(A[v_i0, v_i1], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">transpose</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_transpose: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_transpose&quot;</span>):\n",
              "                v_ax0, v_ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_ax1, v_ax0])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_transpose[v_ax0, v_ax1])\n",
              "                T_transpose[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[v_ax1, v_ax0]\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">transpose1</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_transpose: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_transpose&quot;</span>):\n",
              "                v_ax0, v_ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_ax1, v_ax0])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_transpose[v_ax0, v_ax1])\n",
              "                T_transpose[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[v_ax1, v_ax0]\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">fused_matmul_add0</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
              "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
              "        cls <span style=\"color: #AA22FF; font-weight: bold\">=</span> Module\n",
              "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
              "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul, (x, w), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "            gv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>add, (lv, b), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
              "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">fused_matmul_add1</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">10</span>,), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
              "        R<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;Primitive&quot;</span>: <span style=\"color: #008000\">1</span>})\n",
              "        cls <span style=\"color: #AA22FF; font-weight: bold\">=</span> Module\n",
              "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
              "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>matmul1, (x, w), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "            gv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>add1, (lv, b), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
              "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
              "        cls <span style=\"color: #AA22FF; font-weight: bold\">=</span> Module\n",
              "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
              "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>transpose, (metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>],), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "            lv2: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>fused_matmul_add0(x, lv, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>])\n",
              "            lv3 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu, (lv2,), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "            lv4 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>transpose1, (metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>],), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "            lv6: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>fused_matmul_add1(lv3, lv4, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>])\n",
              "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv6\n",
              "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
              "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
              "\n",
              "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lower fused_matmul\n",
        "MLPModelFinal = relax.transform.FuseTIR()(MLPModelTIR)\n",
        "MLPModelFinal.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hIojC0ux8FOO",
        "outputId": "69c8be03-7595-456b-96c5-308af9dfdf4e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import relax as R</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">fused_matmul_add0</span>(x: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), var_T_add_intermediate: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        var_T_matmul_NN_intermediate <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NN&quot;</span>):\n",
              "                v_i, v_j, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(x[v_i, v_k], w[v_k, v_j])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(var_T_matmul_NN_intermediate[v_i, v_j])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    var_T_matmul_NN_intermediate[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
              "                var_T_matmul_NN_intermediate[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> var_T_matmul_NN_intermediate[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> x[v_i, v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> w[v_k, v_j]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
              "                v_ax0, v_ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(var_T_matmul_NN_intermediate[v_ax0, v_ax1], b[v_ax1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(var_T_add_intermediate[v_ax0, v_ax1])\n",
              "                var_T_add_intermediate[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> var_T_matmul_NN_intermediate[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> b[v_ax1]\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">fused_matmul_add1</span>(x: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), w: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), b: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>),), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), var_T_add_intermediate: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        var_T_matmul_NN_intermediate <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)))\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> i, j, k <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_matmul_NN&quot;</span>):\n",
              "                v_i, v_j, v_k <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SSR&quot;</span>, [i, j, k])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(x[v_i, v_k], w[v_k, v_j])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(var_T_matmul_NN_intermediate[v_i, v_j])\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>init():\n",
              "                    var_T_matmul_NN_intermediate[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>)\n",
              "                var_T_matmul_NN_intermediate[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">=</span> var_T_matmul_NN_intermediate[v_i, v_j] <span style=\"color: #AA22FF; font-weight: bold\">+</span> x[v_i, v_k] <span style=\"color: #AA22FF; font-weight: bold\">*</span> w[v_k, v_j]\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_add&quot;</span>):\n",
              "                v_ax0, v_ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(var_T_matmul_NN_intermediate[v_ax0, v_ax1], b[v_ax1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(var_T_add_intermediate[v_ax0, v_ax1])\n",
              "                var_T_add_intermediate[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> var_T_matmul_NN_intermediate[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">+</span> b[v_ax1]\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">relu</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), compute: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> i0, i1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">1</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;compute&quot;</span>):\n",
              "                v_i0, v_i1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [i0, i1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_i0, v_i1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(compute[v_i0, v_i1])\n",
              "                compute[v_i0, v_i1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>max(A[v_i0, v_i1], T<span style=\"color: #AA22FF; font-weight: bold\">.</span>float32(<span style=\"color: #008000\">0</span>))\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">transpose</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_transpose: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">784</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_transpose&quot;</span>):\n",
              "                v_ax0, v_ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_ax1, v_ax0])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_transpose[v_ax0, v_ax1])\n",
              "                T_transpose[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[v_ax1, v_ax0]\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">transpose1</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), T_transpose: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> ax0, ax1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">128</span>), T<span style=\"color: #AA22FF; font-weight: bold\">.</span>int64(<span style=\"color: #008000\">10</span>)):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;T_transpose&quot;</span>):\n",
              "                v_ax0, v_ax1 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>remap(<span style=\"color: #BA2121\">&quot;SS&quot;</span>, [ax0, ax1])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v_ax1, v_ax0])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(T_transpose[v_ax0, v_ax1])\n",
              "                T_transpose[v_ax0, v_ax1] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[v_ax1, v_ax0]\n",
              "\n",
              "    <span style=\"color: #AA22FF\">@R</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>function\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(x: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">784</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>)) <span style=\"color: #AA22FF; font-weight: bold\">-&gt;</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>):\n",
              "        cls <span style=\"color: #AA22FF; font-weight: bold\">=</span> Module\n",
              "        <span style=\"color: #008000; font-weight: bold\">with</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>dataflow():\n",
              "            lv <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>transpose, (metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">0</span>],), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">784</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "            lv2 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>fused_matmul_add0, (x, lv, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">1</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "            lv3 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>relu, (lv2,), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">128</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "            lv4 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>transpose1, (metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">2</span>],), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">128</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "            lv6 <span style=\"color: #AA22FF; font-weight: bold\">=</span> R<span style=\"color: #AA22FF; font-weight: bold\">.</span>call_tir(cls<span style=\"color: #AA22FF; font-weight: bold\">.</span>fused_matmul_add1, (lv3, lv4, metadata[<span style=\"color: #BA2121\">&quot;relax.expr.Constant&quot;</span>][<span style=\"color: #008000\">3</span>]), out_sinfo<span style=\"color: #AA22FF; font-weight: bold\">=</span>R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>))\n",
              "            gv: R<span style=\"color: #AA22FF; font-weight: bold\">.</span>Tensor((<span style=\"color: #008000\">1</span>, <span style=\"color: #008000\">10</span>), dtype<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;float32&quot;</span>) <span style=\"color: #AA22FF; font-weight: bold\">=</span> lv6\n",
              "            R<span style=\"color: #AA22FF; font-weight: bold\">.</span>output(gv)\n",
              "        <span style=\"color: #008000; font-weight: bold\">return</span> gv\n",
              "\n",
              "<span style=\"color: #007979; font-style: italic\"># Metadata omitted. Use show_meta=True in script() method to show it.</span>\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# build and run"
      ],
      "metadata": {
        "id": "lWIQYnjG8Ijr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "test_data = torchvision.datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=torchvision.transforms.ToTensor()\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "img, label = next(iter(test_loader))\n",
        "img = img.reshape(1, 28, 28).numpy()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(img[0])\n",
        "plt.colorbar()\n",
        "plt.show()\n",
        "\n",
        "print(\"Class:\", class_names[label[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "LXWL02wx8J3u",
        "outputId": "0f879eec-375d-45c8-fd18-7838b10ae8e8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class: Sandal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex = relax.build(MLPModelFinal, target=\"llvm\")\n",
        "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
        "data_nd = tvm.nd.array(img.reshape(1, 784))\n",
        "\n",
        "nd_res = vm[\"main\"](data_nd)\n",
        "\n",
        "pred_kind = np.argmax(nd_res.numpy(), axis=1)\n",
        "print(\"MLPModule Prediction:\", class_names[pred_kind[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOxmCef39KPw",
        "outputId": "07c6a81c-d301-4ea2-ea78-1ad7fe316b71"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLPModule Prediction: Sandal\n"
          ]
        }
      ]
    }
  ]
}