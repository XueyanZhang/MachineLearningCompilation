{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP1GovqDz3yhpWi3efogZAo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XueyanZhang/MachineLearningCompilation/blob/master/7_GPU_Hardware_Acceleration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5Oh_AXY3nQT",
        "outputId": "2c7afd67-4d38-46cf-fbd4-6c5e42e0be86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-nightly-cu121\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_nightly_cu121-0.12.dev1141-cp310-cp310-manylinux_2_28_x86_64.whl (98.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu121) (23.1.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu121) (2.2.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu121) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu121) (0.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu121) (1.22.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu121) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu121) (1.10.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu121) (6.3.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from mlc-ai-nightly-cu121) (4.5.0)\n",
            "Installing collected packages: mlc-ai-nightly-cu121\n",
            "Successfully installed mlc-ai-nightly-cu121-0.12.dev1141\n"
          ]
        }
      ],
      "source": [
        "!python3 -m  pip install mlc-ai-nightly-cu121 -f https://mlc.ai/wheels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python3 -m  pip install mlc-ai-nightly -f https://mlc.ai/wheels"
      ],
      "metadata": {
        "id": "ntLh8cnl5qIe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yPRVNdM4epD",
        "outputId": "a2ce04a5-9e95-4f9e-9a10-2f276ac0bf67"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jun 23 01:44:14 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P0    27W /  70W |    105MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%shell\n",
        "# # Installs the latest dev build of TVM from PyPI, with CUDA enabled. To use this,\n",
        "# # you must request a Google Colab instance with a GPU by going to Runtime ->\n",
        "# # Change runtime type -> Hardware accelerator -> GPU. If you wish to build from\n",
        "# # source, see see https://tvm.apache.org/docs/install/from_source.html\n",
        "# pip install tlcpack-nightly-cu113 --pre -f https://tlcpack.ai/wheels"
      ],
      "metadata": {
        "id": "SA98VL4y4bZ9"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tvm\n",
        "# from tvm import relax\n",
        "from tvm.ir.module import IRModule\n",
        "# from tvm.script import relax as R\n",
        "from tvm.script import tir as T"
      ],
      "metadata": {
        "id": "CD15XRBd5YQj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tvm.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h-z3DHmB3jB5",
        "outputId": "cdbaac6b-af9e-48c0-b9bd-f654e194e4b4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'0.13.dev273+g6b20caee2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Element-wise Add (aka, vector add)\n",
        "\n",
        "An example of GPU programming"
      ],
      "metadata": {
        "id": "meq5rHy3686R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f32 = \"float32\"\n",
        "@tvm.script.ir_module\n",
        "class MyModuleVecAdd:\n",
        "    @T.prim_func\n",
        "    def main(A: T.Buffer((1024,), f32),\n",
        "             B: T.Buffer((1024,), f32),\n",
        "             C: T.Buffer((1024,), f32)) -> None:\n",
        "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
        "        for i in T.grid(1024):\n",
        "            with T.block(\"C\"):\n",
        "                vi = T.axis.remap(\"S\", [i])\n",
        "                C[vi] = A[vi] + B[vi]"
      ],
      "metadata": {
        "id": "Sdns6EQT7fmN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split loop i\n",
        "sch = tvm.tir.Schedule(MyModuleVecAdd)\n",
        "block_C = sch.get_block(\"C\")\n",
        "i, = sch.get_loops(block_C)\n",
        "i0, i1 = sch.split(i, [None, 128])\n",
        "sch.mod.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "tniTKHVkEUyc",
        "outputId": "1f80386f-e074-4aed-8512-ba7f282ef915"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> i_0, i_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>grid(<span style=\"color: #008000\">8</span>, <span style=\"color: #008000\">128</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
              "                vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">128</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i_1)\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[vi], B[vi])\n",
              "                T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[vi])\n",
              "                C[vi] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[vi] <span style=\"color: #AA22FF; font-weight: bold\">+</span> B[vi]\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thread blocks\n",
        "a thread == a core  \n",
        "multiple threads == a thread block  \n",
        "multiple thread blocks == a grid\n",
        "\n",
        "### Identify a thread:\n",
        "- threadIdx.x\n",
        "- blockIdx.x"
      ],
      "metadata": {
        "id": "jwgG3f0TvtFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sch.bind(i0, \"blockIdx.x\")\n",
        "sch.bind(i1, \"threadIdx.x\")\n",
        "sch.mod.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "0YGUxDwmxcep",
        "outputId": "547584e5-c4dd-4946-f474-86b1846be697"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), C: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> i_0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(<span style=\"color: #008000\">8</span>, thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;blockIdx.x&quot;</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> i_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(<span style=\"color: #008000\">128</span>, thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;threadIdx.x&quot;</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
              "                    vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">128</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i_1)\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[vi], B[vi])\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(C[vi])\n",
              "                    C[vi] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[vi] <span style=\"color: #AA22FF; font-weight: bold\">+</span> B[vi]\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# build and run on gpu\n",
        "target = tvm.target.cuda()\n",
        "rt_mod = tvm.build(sch.mod, target=target)\n",
        "\n",
        "A_np = np.random.uniform(size=(1024,)).astype(f32)\n",
        "B_np = np.random.uniform(size=(1024,)).astype(f32)\n",
        "A_nd = tvm.nd.array(A_np, tvm.cuda(0))\n",
        "B_nd = tvm.nd.array(B_np, tvm.cuda(0))\n",
        "C_nd = tvm.nd.array(np.zeros((1024,), dtype=\"float32\"), tvm.cuda(0))\n",
        "\n",
        "rt_mod[\"main\"](A_nd, B_nd, C_nd)\n",
        "print(C_nd)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5ceUeOMx5pi",
        "outputId": "f00cb532-9a7b-465c-ec6a-a97a1272e45b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tvm/target/target.py:418: UserWarning: Try specifying cuda arch by adding 'arch=sm_xx' to your target.\n",
            "  warnings.warn(\"Try specifying cuda arch by adding 'arch=sm_xx' to your target.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.2500075  0.6357553  1.2580568  ... 1.3098538  1.2019022  0.68830264]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Window Sum\n",
        "\n",
        "A basic vervion of \"convolution\"\n",
        "\n",
        "Sliding window sums 3 neighbors."
      ],
      "metadata": {
        "id": "8xhGRwF47cI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tvm.script.ir_module\n",
        "class MyModuleWindowSum:\n",
        "    @T.prim_func\n",
        "    def main(A: T.Buffer((1024,), f32),\n",
        "             B: T.Buffer((1024,), f32)) -> None:\n",
        "        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n",
        "        for i in T.grid(1024):\n",
        "            with T.block(\"C\"):\n",
        "                vi = T.axis.remap(\"S\", [i])\n",
        "                B[vi] = A[vi] + A[vi + 1] + A[vi + 2]"
      ],
      "metadata": {
        "id": "kz4r5L5B7tRO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bind GPU threads\n",
        "sch = tvm.tir.Schedule(MyModuleWindowSum)\n",
        "nthread = 128\n",
        "block_C = sch.get_block(\"C\")\n",
        "i, = sch.get_loops(block_C)\n",
        "i0, i1 = sch.split(i, [None, nthread])\n",
        "sch.bind(i0, \"blockIdx.x\")\n",
        "sch.bind(i1, \"threadIdx.x\")\n",
        "sch.mod.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "7unuhHV08SSY",
        "outputId": "e89f1e08-e583-4e40-ea08-eac6657cca0a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> i_0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(<span style=\"color: #008000\">8</span>, thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;blockIdx.x&quot;</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> i_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(<span style=\"color: #008000\">128</span>, thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;threadIdx.x&quot;</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
              "                    vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">128</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i_1)\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[vi:vi <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">3</span>])\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(B[vi])\n",
              "                    B[vi] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[vi] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[vi <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">1</span>] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A[vi <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">2</span>]\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shared memory\n",
        "\n",
        "Each thread block has a shared memory that all threads (within this blk) can access."
      ],
      "metadata": {
        "id": "Zo1fZXsE_GHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add an intermediate stage (representing the shared memory)\n",
        "A_shared = sch.cache_read(block_C, read_buffer_index=0, storage_scope=\"shared\")\n",
        "# move a block under i1 loop\n",
        "sch.compute_at(A_shared, i1)\n",
        "sch.mod.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "El4YkS06_aOa",
        "outputId": "22fab621-ad25-4f5c-a25e-5e809ccf0086"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        A_shared <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1024</span>,), scope<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;shared&quot;</span>)\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> i_0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(<span style=\"color: #008000\">8</span>, thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;blockIdx.x&quot;</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> i_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(<span style=\"color: #008000\">128</span>, thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;threadIdx.x&quot;</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> ax0 <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">130</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;A_shared&quot;</span>):\n",
              "                        v0 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">128</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax0)\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>where(i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">128</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax0 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">1024</span>)\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v0])\n",
              "                        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(A_shared[v0])\n",
              "                        A_shared[v0] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[v0]\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
              "                    vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">128</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i_1)\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A_shared[vi:vi <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">3</span>])\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(B[vi])\n",
              "                    B[vi] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A_shared[vi] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A_shared[vi <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">1</span>] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A_shared[vi <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">2</span>]\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cooperative fetching\n",
        "\n",
        "threads work together to bring in data"
      ],
      "metadata": {
        "id": "j0DpaDROA63G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = sch.get_loops(A_shared)[-1]\n",
        "ax0, ax1 = sch.split(ax, [None, nthread])\n",
        "sch.bind(ax1, \"threadIdx.x\")\n",
        "sch.mod.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "KuJPCTYDBFCI",
        "outputId": "d58fbb60-4246-4d42-87cf-270a5d1a6b38"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tvm/script/highlight.py:117: UserWarning: No module named 'black'\n",
            "To print formatted TVM script, please install the formatter 'Black':\n",
            "/usr/bin/python3 -m pip install \"black==22.3.0\" --upgrade --user\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div class=\"highlight\" style=\"background: \"><pre style=\"line-height: 125%;\"><span></span><span style=\"color: #007979; font-style: italic\"># from tvm.script import ir as I</span>\n",
              "<span style=\"color: #007979; font-style: italic\"># from tvm.script import tir as T</span>\n",
              "\n",
              "<span style=\"color: #AA22FF\">@I</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>ir_module\n",
              "<span style=\"color: #008000; font-weight: bold\">class</span> <span style=\"color: #0000FF; font-weight: bold\">Module</span>:\n",
              "    <span style=\"color: #AA22FF\">@T</span><span style=\"color: #AA22FF; font-weight: bold\">.</span>prim_func\n",
              "    <span style=\"color: #008000; font-weight: bold\">def</span> <span style=\"color: #0000FF\">main</span>(A: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>), B: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>Buffer((<span style=\"color: #008000\">1024</span>,), <span style=\"color: #BA2121\">&quot;float32&quot;</span>)):\n",
              "        T<span style=\"color: #AA22FF; font-weight: bold\">.</span>func_attr({<span style=\"color: #BA2121\">&quot;global_symbol&quot;</span>: <span style=\"color: #BA2121\">&quot;main&quot;</span>, <span style=\"color: #BA2121\">&quot;tir.noalias&quot;</span>: T<span style=\"color: #AA22FF; font-weight: bold\">.</span>bool(<span style=\"color: #008000; font-weight: bold\">True</span>)})\n",
              "        <span style=\"color: #007979; font-style: italic\"># with T.block(&quot;root&quot;):</span>\n",
              "        A_shared <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>alloc_buffer((<span style=\"color: #008000\">1024</span>,), scope<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;shared&quot;</span>)\n",
              "        <span style=\"color: #008000; font-weight: bold\">for</span> i_0 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(<span style=\"color: #008000\">8</span>, thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;blockIdx.x&quot;</span>):\n",
              "            <span style=\"color: #008000; font-weight: bold\">for</span> i_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(<span style=\"color: #008000\">128</span>, thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;threadIdx.x&quot;</span>):\n",
              "                <span style=\"color: #008000; font-weight: bold\">for</span> ax0_0 <span style=\"color: #008000; font-weight: bold\">in</span> range(<span style=\"color: #008000\">2</span>):\n",
              "                    <span style=\"color: #008000; font-weight: bold\">for</span> ax0_1 <span style=\"color: #008000; font-weight: bold\">in</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>thread_binding(<span style=\"color: #008000\">128</span>, thread<span style=\"color: #AA22FF; font-weight: bold\">=</span><span style=\"color: #BA2121\">&quot;threadIdx.x&quot;</span>):\n",
              "                        <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;A_shared&quot;</span>):\n",
              "                            v0 <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">128</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> (ax0_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">128</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax0_1))\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>where(i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">128</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> (ax0_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">128</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax0_1) <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">1024</span> <span style=\"color: #008000; font-weight: bold\">and</span> ax0_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">128</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> ax0_1 <span style=\"color: #AA22FF; font-weight: bold\">&lt;</span> <span style=\"color: #008000\">130</span>)\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A[v0])\n",
              "                            T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(A_shared[v0])\n",
              "                            A_shared[v0] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A[v0]\n",
              "                <span style=\"color: #008000; font-weight: bold\">with</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>block(<span style=\"color: #BA2121\">&quot;C&quot;</span>):\n",
              "                    vi <span style=\"color: #AA22FF; font-weight: bold\">=</span> T<span style=\"color: #AA22FF; font-weight: bold\">.</span>axis<span style=\"color: #AA22FF; font-weight: bold\">.</span>spatial(<span style=\"color: #008000\">1024</span>, i_0 <span style=\"color: #AA22FF; font-weight: bold\">*</span> <span style=\"color: #008000\">128</span> <span style=\"color: #AA22FF; font-weight: bold\">+</span> i_1)\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>reads(A_shared[vi:vi <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">3</span>])\n",
              "                    T<span style=\"color: #AA22FF; font-weight: bold\">.</span>writes(B[vi])\n",
              "                    B[vi] <span style=\"color: #AA22FF; font-weight: bold\">=</span> A_shared[vi] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A_shared[vi <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">1</span>] <span style=\"color: #AA22FF; font-weight: bold\">+</span> A_shared[vi <span style=\"color: #AA22FF; font-weight: bold\">+</span> <span style=\"color: #008000\">2</span>]\n",
              "</pre></div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspect Lower level Code\n",
        "\n",
        "the code has two parts:\n",
        "1. host code: which calls gpu driver\n",
        "2. kernel code: which runs computations\n",
        "\n"
      ],
      "metadata": {
        "id": "u3lC177HBtuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print out cuda kernel\n",
        "rt_mod = tvm.build(sch.mod, target=\"cuda\")\n",
        "print(rt_mod.imported_modules[0].get_source())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDJJsq_fBsfz",
        "outputId": "21d40214-2273-4a12-a58c-1df728d1fe3a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "#if (((__CUDACC_VER_MAJOR__ == 11) && (__CUDACC_VER_MINOR__ >= 4)) || \\\n",
            "     (__CUDACC_VER_MAJOR__ > 11))\n",
            "#define TVM_ENABLE_L2_PREFETCH 1\n",
            "#else\n",
            "#define TVM_ENABLE_L2_PREFETCH 0\n",
            "#endif\n",
            "\n",
            "#ifdef _WIN32\n",
            "  using uint = unsigned int;\n",
            "  using uchar = unsigned char;\n",
            "  using ushort = unsigned short;\n",
            "  using int64_t = long long;\n",
            "  using uint64_t = unsigned long long;\n",
            "#else\n",
            "  #define uint unsigned int\n",
            "  #define uchar unsigned char\n",
            "  #define ushort unsigned short\n",
            "  #define int64_t long long\n",
            "  #define uint64_t unsigned long long\n",
            "#endif\n",
            "extern \"C\" __global__ void __launch_bounds__(128) main_kernel(float* __restrict__ A, float* __restrict__ B) {\n",
            "  __shared__ float A_shared[1024];\n",
            "  for (int ax0_0 = 0; ax0_0 < 2; ++ax0_0) {\n",
            "    if (((((int)blockIdx.x) + ax0_0) < 8) && (((ax0_0 * 64) + (((int)threadIdx.x) >> 1)) < 65)) {\n",
            "      A_shared[((ax0_0 * 128) + ((int)threadIdx.x))] = A[(((((int)blockIdx.x) * 128) + (ax0_0 * 128)) + ((int)threadIdx.x))];\n",
            "    }\n",
            "  }\n",
            "  __syncthreads();\n",
            "  B[((((int)blockIdx.x) * 128) + ((int)threadIdx.x))] = ((A_shared[((int)threadIdx.x)] + A_shared[(((int)threadIdx.x) + 1)]) + A_shared[(((int)threadIdx.x) + 2)]);\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print metal / opencl kernel\n",
        "rt_mod = tvm.build(sch.mod, target=\"metal\")\n",
        "print(rt_mod.imported_modules[0].get_source())\n",
        "\n",
        "rt_mod = tvm.build(sch.mod, target=\"opencl\")\n",
        "print(rt_mod.imported_modules[0].get_source())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mazNR7T6CL-T",
        "outputId": "5616f07c-10fa-460c-d8b8-832cf9d9f337"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "// Function: main_kernel\n",
            "#include <metal_stdlib>\n",
            "using namespace metal;\n",
            "\n",
            "union __TVMArgUnion {\n",
            " int v_int[2];\n",
            "};\n",
            "\n",
            "kernel void main_kernel(  device float* A [[ buffer(0) ]],\n",
            "  device float* B [[ buffer(1) ]],\n",
            "  uint blockIdx [[threadgroup_position_in_grid]],\n",
            "  uint threadIdx [[thread_position_in_threadgroup]]\n",
            ") {\n",
            "  threadgroup float A_shared[1024];\n",
            "  for (int ax0_0 = 0; ax0_0 < 2; ++ax0_0) {\n",
            "    if (((((int)blockIdx) + ax0_0) < 8) && (((ax0_0 * 64) + (((int)threadIdx) >> 1)) < 65)) {\n",
            "      A_shared[((ax0_0 * 128) + ((int)threadIdx))] = A[(((((int)blockIdx) * 128) + (ax0_0 * 128)) + ((int)threadIdx))];\n",
            "    }\n",
            "  }\n",
            "  threadgroup_barrier(mem_flags::mem_threadgroup);\n",
            "  B[((((int)blockIdx) * 128) + ((int)threadIdx))] = ((A_shared[((int)threadIdx)] + A_shared[(((int)threadIdx) + 1)]) + A_shared[(((int)threadIdx) + 2)]);\n",
            "}\n",
            "\n",
            "\n",
            "\n",
            "// Function: main_kernel\n",
            "__kernel void main_kernel(__global float* restrict A, __global float* restrict B) {\n",
            "  __local float A_shared[1024];\n",
            "  for (int ax0_0 = 0; ax0_0 < 2; ++ax0_0) {\n",
            "    if (((bool)(((convert_int(get_group_id(0))) + ax0_0) < 8)) && ((bool)(((ax0_0 * 64) + ((convert_int(get_local_id(0))) >> 1)) < 65))) {\n",
            "      A_shared[((ax0_0 * 128) + (convert_int(get_local_id(0))))] = A[((((convert_int(get_group_id(0))) * 128) + (ax0_0 * 128)) + (convert_int(get_local_id(0))))];\n",
            "    }\n",
            "  }\n",
            "  barrier(CLK_LOCAL_MEM_FENCE);\n",
            "  B[(((convert_int(get_group_id(0))) * 128) + (convert_int(get_local_id(0))))] = ((A_shared[(convert_int(get_local_id(0)))] + A_shared[((convert_int(get_local_id(0))) + 1)]) + A_shared[((convert_int(get_local_id(0))) + 2)]);\n",
            "}\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}